{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "(train, test), info = tfds.load(\n",
    "    'higgs',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    shuffle_files = True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "valid = train.take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def extract_column(features):\n",
    "    return features[\"class_label\"]  # Change the index to extract the desired feature\n",
    "\n",
    "def extract(features):\n",
    "    return [value for key, value in features.items() if key != \"class_label\"]\n",
    "\n",
    "def recon(data):\n",
    "    return (data.map(extract_column), data.map(extract))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reconfig(dataset):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "\n",
    "    for n in dataset:\n",
    "        xdata.append(list(n.values())[0])\n",
    "        ydata.append(list(n.values())[1:])\n",
    "    \n",
    "    xdata = np.array(xdata)\n",
    "    ydata = np.array(ydata)\n",
    "    return (xdata, ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the input shape (nvis)\n",
    "nvis = 28\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(nvis,)),  # Input layer\n",
    "    layers.Dense(300, activation='tanh', kernel_initializer=keras.initializers.RandomNormal(stddev=0.1), name='h0'),\n",
    "    layers.Dense(300, activation='tanh', kernel_initializer=keras.initializers.RandomNormal(stddev=0.05), name='h1'),\n",
    "    layers.Dense(300, activation='tanh', kernel_initializer=keras.initializers.RandomNormal(stddev=0.05), name='h2'),\n",
    "    layers.Dense(300, activation='tanh', kernel_initializer=keras.initializers.RandomNormal(stddev=0.05), name='h3'),  # Hidden layer\n",
    "    layers.Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.RandomNormal(stddev=0.001), name='y')  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 05:34:41.132629: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Adjusting the data\n",
    "train = train.take(500000)\n",
    "test = test.take(100000)\n",
    "\n",
    "xtrain, ytrain = reconfig(train)\n",
    "xtest, ytest = reconfig(test)\n",
    "xval, yval = reconfig(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "valData = tf.data.Dataset.from_tensor_slices((yval, xval))\n",
    "trainData = tf.data.Dataset.from_tensor_slices((ytrain, xtrain))\n",
    "testData = tf.data.Dataset.from_tensor_slices((ytest, xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class MomentumAdjustor(Callback):\n",
    "    def __init__(self, start=0, saturate=500, final_momentum=0.99):\n",
    "        super(MomentumAdjustor, self).__init__()\n",
    "        self.start = start\n",
    "        self.saturate = saturate\n",
    "        self.final_momentum = final_momentum\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch >= self.start:\n",
    "            # Calculate the new momentum value\n",
    "            momentum = (self.final_momentum - 0) * (epoch - self.start) / self.saturate\n",
    "            momentum = min(momentum, self.final_momentum)  # Clamp to final_momentum\n",
    "            # Update the momentum in the optimizer\n",
    "            self.model.optimizer.momentum = momentum\n",
    "            print(f'\\nEpoch {epoch + 1}: Adjusted momentum to {momentum:.4f}')\n",
    "\n",
    "momentum_adjustor = MomentumAdjustor(start=0.9, saturate=200, final_momentum=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = trainData.batch(batch_size)\n",
    "valid_dataset = valData.batch(batch_size)\n",
    "test_dataset = testData.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.05\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=1.0000002,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, weight_decay=0.00001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['auc', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - accuracy: 0.7442 - auc: 0.8242 - loss: 0.5119 - val_accuracy: 0.7301 - val_auc: 0.8124 - val_loss: 0.5289\n",
      "\n",
      "Epoch 2: Adjusted momentum to 0.0005\n",
      "Epoch 2/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7459 - auc: 0.8258 - loss: 0.5099 - val_accuracy: 0.7279 - val_auc: 0.8121 - val_loss: 0.5284\n",
      "\n",
      "Epoch 3: Adjusted momentum to 0.0054\n",
      "Epoch 3/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7473 - auc: 0.8278 - loss: 0.5074 - val_accuracy: 0.7301 - val_auc: 0.8116 - val_loss: 0.5304\n",
      "\n",
      "Epoch 4: Adjusted momentum to 0.0104\n",
      "Epoch 4/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7483 - auc: 0.8290 - loss: 0.5058 - val_accuracy: 0.7340 - val_auc: 0.8151 - val_loss: 0.5257\n",
      "\n",
      "Epoch 5: Adjusted momentum to 0.0153\n",
      "Epoch 5/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7501 - auc: 0.8308 - loss: 0.5035 - val_accuracy: 0.7322 - val_auc: 0.8137 - val_loss: 0.5315\n",
      "\n",
      "Epoch 6: Adjusted momentum to 0.0203\n",
      "Epoch 6/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7505 - auc: 0.8321 - loss: 0.5020 - val_accuracy: 0.7300 - val_auc: 0.8105 - val_loss: 0.5320\n",
      "\n",
      "Epoch 7: Adjusted momentum to 0.0252\n",
      "Epoch 7/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7525 - auc: 0.8339 - loss: 0.4996 - val_accuracy: 0.7290 - val_auc: 0.8108 - val_loss: 0.5378\n",
      "\n",
      "Epoch 8: Adjusted momentum to 0.0302\n",
      "Epoch 8/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - accuracy: 0.7533 - auc: 0.8353 - loss: 0.4980 - val_accuracy: 0.7312 - val_auc: 0.8122 - val_loss: 0.5313\n",
      "\n",
      "Epoch 9: Adjusted momentum to 0.0351\n",
      "Epoch 9/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7561 - auc: 0.8369 - loss: 0.4961 - val_accuracy: 0.7297 - val_auc: 0.8110 - val_loss: 0.5366\n",
      "\n",
      "Epoch 10: Adjusted momentum to 0.0401\n",
      "Epoch 10/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7564 - auc: 0.8373 - loss: 0.4956 - val_accuracy: 0.7283 - val_auc: 0.8089 - val_loss: 0.5437\n",
      "\n",
      "Epoch 11: Adjusted momentum to 0.0450\n",
      "Epoch 11/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7571 - auc: 0.8384 - loss: 0.4942 - val_accuracy: 0.7262 - val_auc: 0.8075 - val_loss: 0.5418\n",
      "\n",
      "Epoch 12: Adjusted momentum to 0.0500\n",
      "Epoch 12/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - accuracy: 0.7592 - auc: 0.8406 - loss: 0.4915 - val_accuracy: 0.7279 - val_auc: 0.8076 - val_loss: 0.5416\n",
      "\n",
      "Epoch 13: Adjusted momentum to 0.0549\n",
      "Epoch 13/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7589 - auc: 0.8410 - loss: 0.4910 - val_accuracy: 0.7219 - val_auc: 0.8031 - val_loss: 0.5468\n",
      "\n",
      "Epoch 14: Adjusted momentum to 0.0599\n",
      "Epoch 14/500\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7607 - auc: 0.8420 - loss: 0.4898 - val_accuracy: 0.7255 - val_auc: 0.8052 - val_loss: 0.5508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f2ac0376620>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# Set up callbacks for monitoring and learning rate adjustment\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.0000002, patience=5, min_lr=0.000001)\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Train the model\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=500,  # Adjust based on your max epochs criteria\n",
    "    callbacks=[early_stopping, tensorboard_callback, momentum_adjustor]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
